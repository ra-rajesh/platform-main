name: Terraform Destroy

on:
  workflow_dispatch:
    inputs:
      env:
        description: Environment
        required: true
        type: choice
        options: [dev, stage, prod]
        default: stage
      stack:
        description: Stack to destroy (all or a single stack)
        required: true
        type: choice
        options: [all, cloudwatch, rest-api, ssm, nlb, compute, ecr, s3, network]
        default: all

concurrency:
  group: tf-destroy-${{ inputs.env }}-${{ inputs.stack }}-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  destroy:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      AWS_REGION: ap-south-1
      TF_IN_AUTOMATION: "true"
      AWS_ACCESS_KEY_ID_DEV:       ${{ secrets.AWS_ACCESS_KEY_ID_DEV }}
      AWS_SECRET_ACCESS_KEY_DEV:   ${{ secrets.AWS_SECRET_ACCESS_KEY_DEV }}
      AWS_ACCESS_KEY_ID_STAGE:     ${{ secrets.AWS_ACCESS_KEY_ID_STAGE }}
      AWS_SECRET_ACCESS_KEY_STAGE: ${{ secrets.AWS_SECRET_ACCESS_KEY_STAGE }}
      AWS_ACCESS_KEY_ID_PROD:      ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
      AWS_SECRET_ACCESS_KEY_PROD:  ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}

    steps:
      - uses: actions/checkout@v4

      - name: Select AWS access keys for env
        shell: bash
        run: |
          case "${{ inputs.env }}" in
            dev)
              echo "AWS_ACCESS_KEY_ID=${{ env.AWS_ACCESS_KEY_ID_DEV }}" >> $GITHUB_ENV
              echo "AWS_SECRET_ACCESS_KEY=${{ env.AWS_SECRET_ACCESS_KEY_DEV }}" >> $GITHUB_ENV
              ;;
            stage)
              echo "AWS_ACCESS_KEY_ID=${{ env.AWS_ACCESS_KEY_ID_STAGE }}" >> $GITHUB_ENV
              echo "AWS_SECRET_ACCESS_KEY=${{ env.AWS_SECRET_ACCESS_KEY_STAGE }}" >> $GITHUB_ENV
              ;;
            prod)
              echo "AWS_ACCESS_KEY_ID=${{ env.AWS_ACCESS_KEY_ID_PROD }}" >> $GITHUB_ENV
              echo "AWS_SECRET_ACCESS_KEY=${{ env.AWS_SECRET_ACCESS_KEY_PROD }}" >> $GITHUB_ENV
              ;;
          esac

      - name: Configure AWS credentials (access keys)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      - name: Destroy stacks in safe order
        shell: bash
        run: |
          set -euo pipefail

          ENV="${{ inputs.env }}"
          SEL="${{ inputs.stack }}"

          # Safe destroy order (downstream first)
          ORDER=( cloudwatch rest-api ssm nlb compute ecr s3 network )
          if [[ "$SEL" != "all" ]]; then ORDER=("$SEL"); fi

          for S in "${ORDER[@]}"; do
            WD="infra/environments/${ENV}/stacks/${S}"
            if [[ ! -d "$WD" ]]; then
              echo "::warning::Skipping $WD (not found)"
              continue
            fi
            if [[ ! -f "${WD}/backend-${ENV}.hcl" ]]; then
              echo "::error::Missing ${WD}/backend-${ENV}.hcl"
              exit 1
            fi

            # ðŸ§¹ CLEANUP: special handling for certain stacks before destroy

            # 1) COMPUTE â€” delete orphaned instance profile & role to avoid 'EntityAlreadyExists' on future applies
            if [[ "$S" == "compute" ]]; then
              echo "=== CLEANUP (compute): IAM instance profile/role (${ENV})"
              PROFILE_NAME="${ENV}-ec2-ssm-instance-profile"
              ROLE_NAME="${ENV}-ec2-ssm-role"

              if aws iam get-instance-profile --instance-profile-name "$PROFILE_NAME" >/dev/null 2>&1; then
                echo "Found instance profile: $PROFILE_NAME"

                ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
                PROFILE_ARN="arn:aws:iam::${ACCOUNT_ID}:instance-profile/${PROFILE_NAME}"

                # Disassociate from any instances (region-specific)
                ASSOC_IDS="$(aws ec2 describe-iam-instance-profile-associations \
                  --region "${AWS_REGION}" \
                  --filters Name=iam-instance-profile.arn,Values="${PROFILE_ARN}" \
                  --query 'IamInstanceProfileAssociations[].AssociationId' \
                  --output text || true)"
                for AID in $ASSOC_IDS; do
                  echo "Disassociating instance profile: $AID"
                  aws ec2 disassociate-iam-instance-profile --region "${AWS_REGION}" --association-id "$AID" || true
                done

                # Remove any attached roles from the profile
                ROLE_NAMES="$(aws iam get-instance-profile \
                  --instance-profile-name "$PROFILE_NAME" \
                  --query 'InstanceProfile.Roles[].RoleName' \
                  --output text || true)"
                for RN in $ROLE_NAMES; do
                  echo "Removing role $RN from instance profile $PROFILE_NAME"
                  aws iam remove-role-from-instance-profile \
                    --instance-profile-name "$PROFILE_NAME" \
                    --role-name "$RN" || true
                done

                echo "Deleting instance profile: $PROFILE_NAME"
                aws iam delete-instance-profile --instance-profile-name "$PROFILE_NAME" || true
              else
                echo "Instance profile not present: $PROFILE_NAME (nothing to delete)"
              fi

              # Optionally delete the role too (safe if it exists)
              if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
                echo "Cleaning up role: $ROLE_NAME"

                # Detach managed policies
                for P in $(aws iam list-attached-role-policies --role-name "$ROLE_NAME" \
                          --query 'AttachedPolicies[].PolicyArn' --output text || true); do
                  echo "Detaching $P from $ROLE_NAME"
                  aws iam detach-role-policy --role-name "$ROLE_NAME" --policy-arn "$P" || true
                done

                # Delete inline policies
                for IP in $(aws iam list-role-policies --role-name "$ROLE_NAME" \
                          --query 'PolicyNames[]' --output text || true); do
                  echo "Deleting inline policy $IP from $ROLE_NAME"
                  aws iam delete-role-policy --role-name "$ROLE_NAME" --policy-name "$IP" || true
                done

                echo "Deleting role: $ROLE_NAME"
                aws iam delete-role --role-name "$ROLE_NAME" || true
              else
                echo "Role not present: $ROLE_NAME (nothing to delete)"
              fi
            fi

            # 2) NLB â€” delete existing LB & any TGs (defensive cleanup if state is missing)
            if [[ "$S" == "nlb" ]]; then
              echo "=== CLEANUP (nlb): deleting existing NLB & TGs for ${ENV}"
              NLB_NAME="${ENV}-idlms-nlb"

              # Look up LB ARN by name
              LB_ARN="$(aws elbv2 describe-load-balancers \
                --region "${AWS_REGION}" \
                --names "${NLB_NAME}" \
                --query 'LoadBalancers[0].LoadBalancerArn' \
                --output text 2>/dev/null || true)"

              if [[ -n "$LB_ARN" && "$LB_ARN" != "None" ]]; then
                echo "Found NLB: $NLB_NAME ($LB_ARN)"

                # Delete listeners first
                LISTENERS="$(aws elbv2 describe-listeners \
                  --region "${AWS_REGION}" \
                  --load-balancer-arn "${LB_ARN}" \
                  --query 'Listeners[].ListenerArn' \
                  --output text 2>/dev/null || true)"
                for L in $LISTENERS; do
                  echo "Deleting listener: $L"
                  aws elbv2 delete-listener --region "${AWS_REGION}" --listener-arn "$L" || true
                done

                # Delete the NLB
                echo "Deleting load balancer: $LB_ARN"
                aws elbv2 delete-load-balancer --region "${AWS_REGION}" --load-balancer-arn "${LB_ARN}" || true

                # Wait till deleted (best-effort)
                aws elbv2 wait load-balancers-deleted --region "${AWS_REGION}" --load-balancer-arns "${LB_ARN}" || true
              else
                echo "No NLB found named ${NLB_NAME} (skip LB delete)"
              fi

              # Delete TGs that start with "<env>-idlms-nlb-"
              TG_PREFIX="${NLB_NAME}-"
              echo "Scanning for TGs with prefix: ${TG_PREFIX}"
              TG_LIST="$(aws elbv2 describe-target-groups \
                --region "${AWS_REGION}" \
                --query 'TargetGroups[].{n:TargetGroupName,a:TargetGroupArn}' \
                --output text || true)"

              # TG_LIST lines are "name arn"
              while read -r TG_NAME TG_ARN; do
                [[ -z "${TG_NAME:-}" || -z "${TG_ARN:-}" ]] && continue
                if [[ "$TG_NAME" == ${TG_PREFIX}* ]]; then
                  echo "Deleting target group: $TG_NAME ($TG_ARN)"
                  aws elbv2 delete-target-group --region "${AWS_REGION}" --target-group-arn "$TG_ARN" || true
                fi
              done <<< "$TG_LIST"
            fi

            TFVARS_FLAG=""
            if [[ -f "${WD}/${ENV}.tfvars" ]]; then
              TFVARS_FLAG="-var-file=${ENV}.tfvars"
            else
              echo "::notice::No ${WD}/${ENV}.tfvars; relying on defaults/vars"
            fi

            echo "=== INIT: $WD"
            terraform -chdir="$WD" init -input=false -reconfigure -backend-config="backend-${ENV}.hcl"

            echo "=== PLAN (destroy): $WD"
            terraform -chdir="$WD" plan -destroy -input=false $TFVARS_FLAG -var="env_name=${ENV}" -out=tfplan

            echo "=== DESTROY: $WD"
            terraform -chdir="$WD" apply -input=false -auto-approve tfplan || {
              echo "::error::Destroy failed for $WD"
              exit 1
            }
          done
